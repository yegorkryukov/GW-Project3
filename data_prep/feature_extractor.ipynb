{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# mongodb\n",
    "import pymongo as pm\n",
    "\n",
    "# fine-tuned newspaper lib\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "from resources.newspaper.newspaper.article import Article\n",
    "\n",
    "import bs4 as bs\n",
    "from urllib.parse import urljoin\n",
    "from dateutil.parser import parse as date_parser\n",
    "from time import sleep\n",
    "import random\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for infinite scroll page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import sys\n",
    "\n",
    "import unittest, time, re\n",
    "\n",
    "# to divert selenium log stream away\n",
    "logging.getLogger('selenium').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )\n",
    "    log.debug(f'Saved to DB')\n",
    "\n",
    "def scrape(url, db, collection):\n",
    "    '''\n",
    "    Scrapes an article from the 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    date        : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Article's html and features stored to db, \n",
    "    Article's publish date\n",
    "    \n",
    "    '''\n",
    "    log.debug(f\"Exctracting features from {url}\")\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        # the below method may only extract a snippet... \n",
    "        # check the database for results of text extraction\n",
    "        # and apply additional processing if needed after \n",
    "        # article has been stored in the DB\n",
    "        # see code below Newrepublic for example\n",
    "        article.parse()\n",
    "    except Exception as e:\n",
    "        log.critical(f'Data not saved: {e}')\n",
    "        return datetime.datetime.now()\n",
    "    \n",
    "    saveToDB(db, collection, article.url, article.html, meta={\n",
    "        'date'    :article.publish_date,\n",
    "        'title'   :article.title,\n",
    "        'text'    :article.text,\n",
    "        'authors' :article.authors\n",
    "    })\n",
    "    \n",
    "    return article.publish_date\n",
    "\n",
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.objectid import ObjectId\n",
    "\n",
    "def show_doc(db, collection, id):\n",
    "    '''\n",
    "    Finds a document by 'id' and prints contents to the console\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db         : database name\n",
    "    collection : mongodb collection\n",
    "    id         : mongodb document id\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Prints first 100 symbols of each document's key to console\n",
    "    '''\n",
    "    \n",
    "    doc = db[collection].find_one({'_id':ObjectId(id)})\n",
    "    for k in doc:\n",
    "        print(f\"{k} : {str(doc[k])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powered by NewsAPI.org\n",
    "SOURCES = {\n",
    "    'left'  : [\n",
    "        'https://newrepublic.com',\n",
    "        'https://www.motherjones.com'\n",
    "# 3. Slate\n",
    "# 4. The Intercept\n",
    "# 5. Daily Beast\n",
    "# 6. The Atlantic\n",
    "# 7. Washington Post\n",
    "# 8. Politico\n",
    "# 9. The Guardian\n",
    "# 10. BBC\n",
    "    ],\n",
    "    'right' : [\n",
    "        'https://www.breitbart.com'\n",
    "# 2. Fox News\n",
    "# 3. New York Post\n",
    "# 4. The American Conservative\n",
    "# 5. Washington Times\n",
    "# 6. Daily Wire\n",
    "# 7. The Fiscal Times\n",
    "# 8. The Hill\n",
    "# 9. The Daily Caller\n",
    "# 10. Reason\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Some comands to keep dbs clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# deletes all 'meta' fields from all docs\n",
    "# htmlCol.update({}, {$unset: {meta:1}}, false, true); # mongo shell comand\n",
    "htmlCol.update({}, {'$unset': {'meta':1}}, multi=True) # pymongo way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# leaves only unique documents by 'url' field\n",
    "\n",
    "htmlCol.create_index(\n",
    "    \"url\",\n",
    "    unique=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pymongo 'find' returns cursor that allows iterating through results\n",
    "# calling first object [0] allows accessing the dictionary with results\n",
    "# the ['html'] is the key in the dictionary\n",
    "html = htmlCol.find({'url':'http://www.msnbc.com/velshi-ruhle/watch/jeff-sessions-is-justifying-harsh-immigration-policy-with-the-bible-1256689731629'},\\\n",
    "            projection={'html':True, '_id':False})[0]['html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find documents NOT containing a 'tag': regex expression\n",
    "import re\n",
    "tag = re.compile('dek___3AQpw.')\n",
    "docs = htmlCol.find({\"html\" : {'$not': tag}})\n",
    "for d in docs[:20]: print(d['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "# use multiprocessing to extract features\n",
    "def func():\n",
    "    DB_NAME = 'scrape'\n",
    "    db = pm.MongoClient(host='localhost', port=27017, maxPoolSize=500)[DB_NAME]\n",
    "\n",
    "    for collection in ['left','right']: docs_parser(db[collection])\n",
    "\n",
    "proc = Process(target=func)\n",
    "proc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newrepublic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of collection for this media\n",
    "collection = 'newRep'\n",
    "source = 'https://newrepublic.com/latest'\n",
    "page   = 91\n",
    "\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "\n",
    "while True:\n",
    "    log.debug(f'PROCESSING PAGE: {page}')\n",
    "    s = Source(source+'?page='+str(page))\n",
    "    s.download()\n",
    "\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for section in soup.findAll('article'):\n",
    "        url = urljoin(s.url, section.a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        article_date = scrape(url, db, collection)\n",
    "\n",
    "    if article_date < earliest_date:\n",
    "        log.debug(f'Reached earliest date requested: {article_date}')\n",
    "        break\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update text field to include more data\n",
    "for doc in db[collection].find():\n",
    "    soup = bs.BeautifulSoup(doc['html'],'lxml')\n",
    "    text = ''\n",
    "    for div in soup.findAll('div',{\"class\": \"content-body\"}):\n",
    "        text += div.text\n",
    "    if len(doc['meta']['text']) < len(text):\n",
    "        db[collection].update_one(\n",
    "            {'url' : doc['url']},\n",
    "            {\n",
    "                '$set':\n",
    "                    {\n",
    "                     'meta.text' : text\n",
    "                    }\n",
    "            }\n",
    "            ,\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "url = 'https://newrepublic.com/article/139550/legacy-altamont'\n",
    "doc = db[collection].find_one({'url':url})\n",
    "doc['meta']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utc=pytz.UTC\n",
    "\n",
    "# name of collection for this media\n",
    "collection = 'theAtlantic'\n",
    "source = 'https://www.theatlantic.com/latest/'\n",
    "\n",
    "# start 'page' at '1' but if you run across an error\n",
    "# efficient way is to update this page to the same number\n",
    "# where you experienced the error AFTER you correct the error in the code\n",
    "# then rerun this cell\n",
    "page   = 175\n",
    "\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+'?page='+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for section in soup.findAll('li', {\"class\":\"article\"}):\n",
    "        url = urljoin(s.url, section.a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        try:\n",
    "            article_date = scrape(url, db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1 #to make sure scraping continues\n",
    "        \n",
    "        # the Atlantic blocks right away after few quick downloads\n",
    "        # so it requires sleeping, testing showed 1 to 5 seconds is enough\n",
    "#         sleep(random.uniform(1,5))\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "# use your newsapi key\n",
    "# update collection, date and source\n",
    "collection = 'Politoco'\n",
    "page = 1\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date = '2017-09-13'\n",
    "source = 'politico'\n",
    "pageSize = 100\n",
    "apiKey = 'fd1386b678fd4524b2aa84e3bee1f8f7'\n",
    "\n",
    "params = {\n",
    "    'apiKey' : apiKey,\n",
    "    'pageSize' : pageSize,\n",
    "    'page' : page,\n",
    "    'from' : earliest_date,\n",
    "    'to'   : latest_date,\n",
    "    'sources': source\n",
    "}\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# get first request to obtain total news count\n",
    "r = requests.get(api_url, params=params)\n",
    "totalPages = r.json()['totalResults']//100\n",
    "log.debug(f'Total pages ({pageSize} per page) for {source} results from {earliest_date} to {latest_date} is {totalPages}')\n",
    "\n",
    "# scrape news\n",
    "for p in range(page,totalPages+1):\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    \n",
    "    params = {\n",
    "        'apiKey' : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page' : page,\n",
    "        'from' : earliest_date,\n",
    "        'to'   : latest_date,\n",
    "        'sources': source\n",
    "    }\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    if r.json()['articles']:\n",
    "        for a in r.json()['articles']:\n",
    "            try:\n",
    "                log.debug(f\"Processing url: {a['url']}\")\n",
    "                article_date = scrape(a['url'], db, collection)\n",
    "            except Exception:\n",
    "                article_date = earliest_date + 1\n",
    "    else:\n",
    "        log.debug(f\"Response doesn't have articles. Response: {r.json()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.json()['totalResults']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Washington Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "# use your newsapi key\n",
    "# update collection, date and source\n",
    "collection = 'WashTimes'\n",
    "\n",
    "page = 1\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date = '2017-12-31'\n",
    "source = 'the-washington-times'\n",
    "pageSize = 100\n",
    "apiKey = 'fd1386b678fd4524b2aa84e3bee1f8f7'\n",
    "\n",
    "params = {\n",
    "    'apiKey'   : apiKey,\n",
    "    'pageSize' : pageSize,\n",
    "    'page'     : page,\n",
    "    'from'     : earliest_date,\n",
    "    'to'       : latest_date,\n",
    "    'sources'  : source\n",
    "}\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# get first request to obtain total news count\n",
    "r = requests.get(api_url, params=params)\n",
    "totalPages = r.json()['totalResults']//100\n",
    "\n",
    "log.debug(f'TOTAL PAGES FOR {source}: {totalPages}')\n",
    "\n",
    "# scrape news\n",
    "for p in range(page,totalPages):\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    \n",
    "    params = {\n",
    "        'apiKey'   : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page'     : page,\n",
    "        'from'     : earliest_date,\n",
    "        'to'       : latest_date,\n",
    "        'sources'  : source\n",
    "    }\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    for a in r.json()['articles']:\n",
    "        try:\n",
    "            log.debug(f\"Processing url: {a['url']}\")\n",
    "            if db[collection].find({'url':a['url']}):\n",
    "                log.debug(f'URL exists in DB. Skipping.')\n",
    "            else:\n",
    "                article_date = scrape(a['url'], db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'slate'\n",
    "source = 'https://slate.com/news-and-politics/'\n",
    "page   = 118\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for link in soup.find('div', {\"class\":\"topic-stories-list\"}).findChildren(\"a\" , recursive=False):\n",
    "        url = link['href']\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        try:\n",
    "            article_date = scrape(url, db, collection)\n",
    "        except Exception:\n",
    "            article_date = earliest_date + 1 #to make sure scraping continues\n",
    "        \n",
    "        # the Atlantic blocks right away after few quick downloads\n",
    "        # so it requires sleeping, testing showed 1 to 5 seconds is enough\n",
    "#         sleep(random.uniform(1,5))\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "collection = 'theintercept'\n",
    "source = 'https://theintercept.com'\n",
    "\n",
    "logging.getLogger('selenium').setLevel(logging.WARNING)\n",
    "\n",
    "class Sel(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.implicitly_wait(30)\n",
    "        self.base_url = source\n",
    "        self.verificationErrors = []\n",
    "        self.accept_next_alert = True\n",
    "    def getPage(self):\n",
    "        driver = self.driver\n",
    "        delay = 2\n",
    "        driver.get(self.base_url)\n",
    "        html_source = driver.page_source\n",
    "        self.html = html_source.encode('utf-8')\n",
    "        return self.html\n",
    "    def scrollDown(self):\n",
    "        driver = self.driver\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        self.html = driver.page_source.encode('utf-8')\n",
    "        return driver.page_source \n",
    "    def shutdown(self):\n",
    "        driver = self.driver\n",
    "        driver.quit()\n",
    "\n",
    "data = Sel()\n",
    "\n",
    "data.setUp()\n",
    "data.getPage()\n",
    "page = 1\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = utc.localize(date_parser('2017-01-01'))\n",
    "\n",
    "scraped_urls = []\n",
    "\n",
    "while True:\n",
    "    log.debug(f'NEXT SCROLL (#{page})')\n",
    "    page += 1\n",
    "    \n",
    "    html = data.scrollDown()\n",
    "    soup = bs.BeautifulSoup(html,'lxml')\n",
    "\n",
    "    for link in soup.find('div', {\"id\":\"recently\"}).findAll('a'):\n",
    "        url = urljoin(source, link['href'])\n",
    "        if url and url not in scraped_urls:\n",
    "            scraped_urls.append(url)\n",
    "            log.debug(f'Processing url: {url}')\n",
    "            article_date = scrape(url, db, collection)\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f\"Something is wrong: {e}\")\n",
    "data.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The daily beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "collection = 'thedailybeast'\n",
    "source = 'https://www.thedailybeast.com/sitemap'\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "s = Source(source)\n",
    "s.download()\n",
    "soup = bs.BeautifulSoup(s.html,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "months = ['https://www.thedailybeast.com/sitemap/2018/1/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/2/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/3/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/4/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/5/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/6/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2018/7/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/1/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/2/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/3/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/4/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/5/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/6/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/7/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/8/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/9/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/10/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/11/article',\n",
    " 'https://www.thedailybeast.com/sitemap/2017/12/article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for month in months:\n",
    "    log.debug(f\"\\n=======PROCESSING PAGE: {month}\\n\")\n",
    "    s = Source(month)\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "    for a in soup.find('div',{'class':'SitemapMonthPage__articles-wrapper'}).findAll('a'):\n",
    "        url = urljoin(source, a['href'])\n",
    "        log.debug(f'Processing url: {url}')\n",
    "        \n",
    "        article_date = scrape(url, db, collection) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "collection    = 'theguardian'\n",
    "source        = 'https://content.guardianapis.com/search'\n",
    "apikey        = '53aee7b4-8c94-495b-b13b-ea7214ba0ca2'\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date   = '2017-02-26'\n",
    "section       = 'us-news'\n",
    "edition       = 'us'\n",
    "page          = 1\n",
    "page_size     = 200\n",
    "\n",
    "params = {\n",
    "    'api-key'   : apikey,\n",
    "    'edition'   : edition,\n",
    "    'section'   : section,\n",
    "    'from-date' : earliest_date,\n",
    "    'to-date'   : latest_date,\n",
    "    'page-size' : page_size,\n",
    "    'page'      : page\n",
    "}\n",
    "\n",
    "r = requests.get(source, params=params)\n",
    "\n",
    "response = r.json()['response']\n",
    "pages    = response['pages']\n",
    "\n",
    "for page in range(1,pages):\n",
    "    log.debug(f\"\\n================================\\nPROCESSING PAGE: {page}\\n\")\n",
    "    \n",
    "    params = {\n",
    "    'api-key'   : apikey,\n",
    "    'edition'   : edition,\n",
    "    'section'   : section,\n",
    "    'from-date' : earliest_date,\n",
    "    'to-date'   : latest_date,\n",
    "    'page-size' : page_size,\n",
    "    'page'      : page\n",
    "    }\n",
    "    \n",
    "    r = requests.get(source, params=params)\n",
    "    response = r.json()['response']\n",
    "    \n",
    "    for article in response['results']:\n",
    "        apiUrl = article['apiUrl']\n",
    "        log.debug(f'Processing apiUrl: {apiUrl}')\n",
    "        a = requests.get(apiUrl, params={'api-key':apikey,'show-fields': 'all'}).json()['response']['content']\n",
    "\n",
    "        html  = a['fields']['main'] + a['fields']['body']\n",
    "        url   = a['webUrl']\n",
    "        log.debug(f'Weburl: {url}')\n",
    "        \n",
    "        try:\n",
    "            saveToDB(db, collection, url, html, meta={\n",
    "                'date'    : date_parser(a['fields']['firstPublicationDate']),\n",
    "                'title'   : a['webTitle'],\n",
    "                'text'    : a['fields']['bodyText'],\n",
    "                'apiURL'  : url\n",
    "            })  \n",
    "        except Exception as e:\n",
    "            log.debug(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fox news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "# use your newsapi key\n",
    "# update collection, date and source\n",
    "collection = 'foxnews'\n",
    "stopWords = ['video','radio','entertainment','lifestyle','lifestyle']\n",
    "\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "page = 1\n",
    "earliest_date = '2017-01-01'\n",
    "latest_date = '2017-09-12'\n",
    "source = 'fox-news'\n",
    "pageSize = 100\n",
    "apiKey = 'fd1386b678fd4524b2aa84e3bee1f8f7'\n",
    "\n",
    "params = {\n",
    "    'apiKey'   : apiKey,\n",
    "    'pageSize' : pageSize,\n",
    "    'page'     : page,\n",
    "    'from'     : earliest_date,\n",
    "    'to'       : latest_date,\n",
    "    'sources'  : source\n",
    "}\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'\n",
    "\n",
    "# get first request to obtain total news count\n",
    "r = requests.get(api_url, params=params)\n",
    "totalPages = r.json()['totalResults']//100+1\n",
    "\n",
    "log.debug(f'TOTAL PAGES FOR {source}: {totalPages}')\n",
    "\n",
    "# scrape news\n",
    "for p in range(page,totalPages):\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+\"?page=\"+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    \n",
    "    params = {\n",
    "        'apiKey'   : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page'     : page,\n",
    "        'from'     : earliest_date,\n",
    "        'to'       : latest_date,\n",
    "        'sources'  : source\n",
    "    }\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "\n",
    "    page += 1\n",
    "    \n",
    "    for a in r.json()['articles']:\n",
    "        try:\n",
    "            skip = False #flag to skip scraping\n",
    "            url = a['url']\n",
    "            log.debug(f\"Processing url: {url}\")\n",
    "            for stopWord in stopWords:\n",
    "                if stopWord in url.lower():\n",
    "                    log.debug(f'URL has {stopWord}, skipping.')\n",
    "                    skip = True\n",
    "            if not skip:      \n",
    "                article_date = scrape(url, db, collection)\n",
    "        except Exception as e:\n",
    "            log.debug(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log.debug(f\"Stopped scraping at page {page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# update text field to include more data\n",
    "for doc in db[collection].find():\n",
    "    soup = bs.BeautifulSoup(doc['html'],'lxml')\n",
    "    text = ''\n",
    "    for div in soup.findAll('div',{\"class\": \"content-body\"}):\n",
    "        text += div.text\n",
    "    if len(doc['meta']['text']) < len(text):\n",
    "        db[collection].update_one(\n",
    "            {'url' : doc['url']},\n",
    "            {\n",
    "                '$set':\n",
    "                    {\n",
    "                     'meta.text' : text\n",
    "                    }\n",
    "            }\n",
    "            ,\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "url = 'https://newrepublic.com/article/139550/legacy-altamont'\n",
    "doc = db[collection].find_one({'url':url})\n",
    "doc['meta']['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
