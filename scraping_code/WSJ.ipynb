{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# mongodb\n",
    "import pymongo as pm\n",
    "\n",
    "# fine-tuned newspaper lib\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "from resources.newspaper.newspaper.article import Article\n",
    "\n",
    "import bs4 as bs\n",
    "from urllib.parse import urljoin\n",
    "from dateutil.parser import parse as date_parser\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "\n",
    "from resources.config import apiKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )\n",
    "    log.debug(f'Saved to DB')\n",
    "\n",
    "def scrape(url, db, collection):\n",
    "    '''\n",
    "    Scrapes an article from the 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    date        : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Article's html and features stored to db, \n",
    "    Article's publish date\n",
    "    \n",
    "    '''\n",
    "    log.debug(f\"Exctracting features from {url}\")\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        # the below method may only extract a snippet... \n",
    "        # check the database for results of text extraction\n",
    "        # and apply additional processing if needed after \n",
    "        # article has been stored in the DB\n",
    "        # see code below Newrepublic for example\n",
    "        article.parse()\n",
    "    except Exception as e:\n",
    "        log.critical(f'Data not saved: {e}')\n",
    "        return datetime.datetime.now()\n",
    "    \n",
    "    saveToDB(db, collection, article.url, article.html, meta={\n",
    "        'date'    :article.publish_date,\n",
    "        'title'   :article.title,\n",
    "        'text'    :article.text,\n",
    "        'authors' :article.authors\n",
    "    })\n",
    "    \n",
    "    return article.publish_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup newsapi.org credentials\n",
    "collection = 'wsj'\n",
    "\n",
    "logging.getLogger('urllib3').setLevel(logging.WARNING)\n",
    "\n",
    "page     = 1\n",
    "source   = 'the-wall-street-journal'\n",
    "pageSize = 100\n",
    "\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "latest_date = date_parser('2018-07-31')\n",
    "\n",
    "params = {\n",
    "        'apiKey'   : apiKey,\n",
    "        'pageSize' : pageSize,\n",
    "        'page'     : page,\n",
    "        'from'     : earliest_date,\n",
    "        'to'       : latest_date,\n",
    "        'sources'  : source\n",
    "    }\n",
    "\n",
    "# base url\n",
    "api_url = 'https://newsapi.org/v2/everything?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Requesting period: 2018-07-01 00:00:00-2018-07-31 00:00:00\n",
      "DEBUG:__main__:Ended scrape\n"
     ]
    }
   ],
   "source": [
    "# scrape news\n",
    "while latest_date > earliest_date:\n",
    "    log.debug(f'Requesting period: {latest_date - timedelta(30)}-{latest_date}')\n",
    "       \n",
    "    page = 1\n",
    "    params['from'] = latest_date - timedelta(30)\n",
    "    params['to']   = latest_date\n",
    "    \n",
    "    r = requests.get(api_url, params=params)\n",
    "    break\n",
    "\n",
    "    totalPages = r.json()['totalResults']//100+1\n",
    "\n",
    "    log.debug(f'TOTAL PAGES FOR {source}: {totalPages}')\n",
    "    \n",
    "    for p in range(page,totalPages):\n",
    "        log.debug(f'\\n\\n PROCESSING PAGE: {page} of {totalPages}\\n')\n",
    "\n",
    "        params['page'] = page\n",
    "        page += 1\n",
    "\n",
    "        r = requests.get(api_url, params=params)\n",
    "\n",
    "        for a in r.json()['articles']:\n",
    "            try:\n",
    "                url = a['url']\n",
    "                log.debug(f\"Processing url: {url}\")\n",
    "                result = requests.get(url)\n",
    "                soup = bs.BeautifulSoup(result.text, 'lxml')\n",
    "                text = soup.find('article').text\n",
    "\n",
    "                saveToDB(db, collection, url, result.text, meta={\n",
    "                    'date'    : date_parser(a['publishedAt']),\n",
    "                    'title'   : a['title'],\n",
    "                    'text'    : text,\n",
    "                    'authors' : a['author']\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                log.debug(e)\n",
    "\n",
    "    latest_date -= timedelta(30)\n",
    "            \n",
    "log.debug('Ended scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': {'id': 'the-wall-street-journal',\n",
       "  'name': 'The Wall Street Journal'},\n",
       " 'author': 'Anthony Harrup',\n",
       " 'title': 'The Wall Street Journal: No deaths reported in Mexico airliner crash',\n",
       " 'description': 'An Aeromexico airliner has crashed in the northern Durango state of Mexico, however the state’s governor says no one was killed.',\n",
       " 'url': 'https://www.wsj.com/articles/airliner-crashes-in-mexico-no-reported-deaths-1533076351',\n",
       " 'urlToImage': 'http://s.marketwatch.com/public/resources/MWimages/MW-GN527_durang_MG_20180731194618.jpg',\n",
       " 'publishedAt': '2018-07-31T23:49:59Z'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()['articles'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.wsj.com/articles/airliner-crashes-in-mexico-no-reported-deaths-1533076351'\n",
    "r = requests.get(url)\n",
    "soup = bs.BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"\">August 1, 2018</p>,\n",
       " <p>An Aeromexico airliner crashed during takeoff Tuesday in the northern Mexican state of Durango, causing a number of injuries but no deaths, the airline and authorities said.</p>,\n",
       " <p>Aeromexico said Flight 2431, an Embraer 190 plane with 97 passengers and four crew members aboard, was heading from Durango City to Mexico City. </p>,\n",
       " <p>“At the moment we don’t...\n",
       "   </p>,\n",
       " <p class=\"style__column-name_2q_SeZUL5gpEK9-5nObB5R \">WSJ Membership</p>,\n",
       " <p class=\"style__column-name_2q_SeZUL5gpEK9-5nObB5R \">Customer Service</p>,\n",
       " <p class=\"style__column-name_2q_SeZUL5gpEK9-5nObB5R \">Tools &amp; Features</p>,\n",
       " <p class=\"style__column-name_2q_SeZUL5gpEK9-5nObB5R \">Ads</p>,\n",
       " <p class=\"style__column-name_2q_SeZUL5gpEK9-5nObB5R \">More</p>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIA_TWITTER = [\"wsj.com\"]\n",
    "\n",
    "def changeRefer(details)\n",
    "\n",
    "    foundReferer = False\n",
    "    foundUA = False\n",
    "\n",
    "    def useTwitter = VIA_TWITTER.map(function(url) {\n",
    "    if (details.url.includes(url)) {\n",
    "      return true;\n",
    "    }\n",
    "    return false;\n",
    "  })\n",
    "  .reduce(function(a, b) { return a || b}, false);\n",
    "\n",
    "  var reqHeaders = details.requestHeaders.filter(function(header) {\n",
    "\n",
    "    // block cookies by default\n",
    "    if (header.name !== \"Cookie\") {\n",
    "      return header;\n",
    "    } \n",
    "\n",
    "  }).map(function(header) {\n",
    "    \n",
    "    if (header.name === \"Referer\") {\n",
    "      header.value = setRefer(useTwitter);\n",
    "      foundReferer = true;\n",
    "    }\n",
    "    if (header.name === \"User-Agent\") {\n",
    "      header.value = setUserAgent(useTwitter);\n",
    "      foundUA = true;\n",
    "    }\n",
    "    return header;\n",
    "  })\n",
    "  \n",
    "  // append referer\n",
    "  if (!foundReferer) {\n",
    "    reqHeaders.push({\n",
    "      \"name\": \"Referer\",\n",
    "      \"value\": setRefer(useTwitter)\n",
    "    })\n",
    "  }\n",
    "  if (!foundUA) {\n",
    "    reqHeaders.push({\n",
    "      \"name\": \"User-Agent\",\n",
    "      \"value\": setUserAgent(useTwitter)\n",
    "    })\n",
    "  }\n",
    "  return {requestHeaders: reqHeaders};\n",
    "}\n",
    "\n",
    "function blockCookies(details) {\n",
    "  for (var i = 0; i < details.responseHeaders.length; ++i) {\n",
    "    if (details.responseHeaders[i].name === \"Set-Cookie\") {\n",
    "      details.responseHeaders.splice(i, 1);\n",
    "    }\n",
    "  }\n",
    "  return {responseHeaders: details.responseHeaders};\n",
    "}\n",
    "\n",
    "function setRefer(useTwitter) {\n",
    "  if (useTwitter) return \"https://t.co/T1323aaaa\"; \n",
    "  else return \"https://www.google.com/\";\n",
    "}\n",
    "\n",
    "function setUserAgent(useTwitter) {\n",
    "  if (useTwitter) return \"Mozilla/5.0 (iPhone; CPU iPhone OS 10_2 like Mac OS X) AppleWebKit/602.1.32 (KHTML, like Gecko) Mobile/14C92 Twitter for iPhone\";\n",
    "  else return \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\";\n",
    "}\n",
    "\n",
    "chrome.webRequest.onBeforeSendHeaders.addListener(changeRefer, {\n",
    "  urls: [\"<all_urls>\"],\n",
    "  types: [\"main_frame\"],\n",
    "}, [\"requestHeaders\", \"blocking\"]);\n",
    "\n",
    "chrome.webRequest.onHeadersReceived.addListener(blockCookies, {\n",
    "  urls: [\"<all_urls>\"],\n",
    "  types: [\"main_frame\"],\n",
    "}, [\"responseHeaders\", \"blocking\"]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
