{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pymongo as pm\n",
    "\n",
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for News Source Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Republic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"New_Republic\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 24)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Daily Wire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Daily_Wire\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 26)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fiscal Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Fiscal_Times\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 30)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mother Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Mother_Jones\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 28)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Daily Beast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Daily_Beast\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 29)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Intercept\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 25)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Washington Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Washington_Times\"].find()\n",
    "for doc in docs:\n",
    "    if \"rss\" in doc['url']:\n",
    "        print(doc['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Guardian\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 28)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Economist\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    start_point = doc['url'].find(\".\")\n",
    "    stop_point = doc['url'].find(\"/\", 26)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])\n",
    "    elif doc['url'][start_point:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fox News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Fox_News\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 23)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"New_York_Post\"].find()\n",
    "count = 1\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 19)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Politico\"].find()\n",
    "for doc in docs:\n",
    "    start_point = doc['url'].find(\".\")\n",
    "    stop_point = doc['url'].find(\"/\", 25)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])\n",
    "    elif doc['url'][start_point:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Slate\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 18)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Atlantic\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 28)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Hill\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 19)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breitbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Breitbart\"].find()\n",
    "for doc in docs:\n",
    "    start_point = doc['url'].find(\".\")\n",
    "    stop_point = doc['url'].find(\"/\", 26)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])\n",
    "    elif doc['url'][start_point:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][start_point:stop_point])\n",
    "        print(doc['url'][start_point:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Daily Caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"The_Daily_Caller\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 23)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"BBC\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 20)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The American Conservative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"American_Conservative\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 39)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Washington_Post\"].find()\n",
    "for doc in docs:\n",
    "    start_point = doc['url'].find(\":\")\n",
    "    stop_point = doc['url'].find(\"/\", 31)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][start_point + 3:stop_point])\n",
    "        print(doc['url'][start_point + 3:stop_point])\n",
    "    elif doc['url'][start_point + 3:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][start_point + 3:stop_point])\n",
    "        print(doc['url'][start_point + 3:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Huffington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"Huff_Post\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 31)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])\n",
    "    elif doc['url'][:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][:stop_point])\n",
    "        print(doc['url'][:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bases = []\n",
    "docs = db[\"New_York_Times\"].find()\n",
    "for doc in docs:\n",
    "    stop_point = doc['url'].find(\"/\", 36)\n",
    "    if len(url_bases) == 0:\n",
    "        url_bases.append(doc['url'][35:stop_point])\n",
    "        print(doc['url'][35:stop_point])\n",
    "    elif doc['url'][35:stop_point] in url_bases:\n",
    "        continue\n",
    "    else:\n",
    "        url_bases.append(doc['url'][35:stop_point])\n",
    "        print(doc['url'][35:stop_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Unneeded Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"The_Guardian\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"theguardian.com/theobserver\" in doc['url']:\n",
    "        db.The_Guardian.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"theguardian.com/culture\" in doc['url']:\n",
    "        db.The_Guardian.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.The_Guardian.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fox News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Fox_News\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"press.foxnews.com\" in doc['url']:\n",
    "        db.Fox_News.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Fox_News.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"The_Reason\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"reason.com/archive\" not in doc['url'] and \"reason.com/blog\" not in doc['url']:\n",
    "        db.The_Reason.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.The_Reason.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Economist\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"economist.com/the-world-if\" in doc['url']:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"economist.com/indicator\" in doc['url']:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"economist.com/puteen\" in doc['url']:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"economist.com/transparency201\" in doc['url']:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"economist.com/node\" in doc['url']:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Economist.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"The_Hill\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"thehill.com/homenews/sunday-talk-show\" in doc['url']:\n",
    "        db.The_Hill.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"thehill.com/homenews/campaign-poll\" in doc['url']:\n",
    "        db.The_Hill.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.The_Hill.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New York Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"New_York_Post\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"pagesix.com\" in doc['url']:\n",
    "        db.New_York_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.New_York_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Washington_Post\"].find(no_cursor_timeout=True)\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"washingtonpost.com/powerpost\" in doc['url']:\n",
    "        db.Washington_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"washingtonpost.com/outlook\" in doc['url']:\n",
    "        db.Washington_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"washingtonpost.com/express\" in doc['url']:\n",
    "        db.Washington_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"washingtonpost.com/posteverything\" in doc['url']:\n",
    "        db.Washington_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Washington_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "docs.close()\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Politico\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"politico.com/tipsheet\" in doc['url']:\n",
    "        db.Politico.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"politico.com/latest-news-update\" in doc['url']:\n",
    "        db.Politico.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"politico.com/election-results\" in doc['url']:\n",
    "        db.Politico.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Politico.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Slate\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if \"slate.com/culture\" in doc['url']:\n",
    "        db.Slate.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif doc['url'] in urls:\n",
    "        db.Slate.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Washington Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Washington_Times\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if \"rss\" in doc['url']:\n",
    "        db.Washington_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif doc['url'] in urls:\n",
    "        db.Washington_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Huffington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Huff_Post\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"huffingtonpost.co\" not in doc['url']:\n",
    "        db.Huff_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Huff_Post.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"New_York_Times\"].find(no_cursor_timeout=True)\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"/sports/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/opinion/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/arts/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/obituaries/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/movies/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/fashion/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/dining/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/style/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/realestate/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/theater/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/admin/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/podcasts/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/crosswords/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/automobiles/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/jobs/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/homepage/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/books/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"/travel/\" in doc['url']:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.New_York_Times.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "docs.close()\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"The_Atlantic\"].find()\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"atlantic.com/personal\" in doc['url']:\n",
    "        db.The_Atlantic.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.The_Atlantic.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breitbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "docs = db[\"Breitbart\"].find(no_cursor_timeout=True)\n",
    "count = 1\n",
    "del_count = 0\n",
    "for doc in docs:\n",
    "    if len(urls) == 0:\n",
    "        urls.append(doc['url'])\n",
    "    elif \"breitbart.com/big-hollywood\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/faith\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/medicine\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/pre-viral\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/weird-news\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/live\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/blog\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/radio\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/video\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif \"breitbart.com/sports\" in doc['url']:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    elif doc['url'] in urls:\n",
    "        db.Breitbart.delete_one({'url': doc['url']})\n",
    "        print(f\"Deleting article {count}\")\n",
    "        count += 1\n",
    "        del_count += 1\n",
    "    else:\n",
    "        urls.append(doc['url'])\n",
    "        print(f\"Processing article {count}\")\n",
    "        count += 1\n",
    "\n",
    "docs.close()\n",
    "print(\"------------\")\n",
    "print(f\"Deleted {del_count} articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
