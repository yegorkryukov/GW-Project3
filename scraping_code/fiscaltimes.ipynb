{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# mongodb\n",
    "import pymongo as pm\n",
    "\n",
    "# fine-tuned newspaper lib\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "from resources.newspaper.newspaper.article import Article\n",
    "\n",
    "import bs4 as bs\n",
    "from urllib.parse import urljoin\n",
    "from dateutil.parser import parse as date_parser\n",
    "from time import sleep\n",
    "import random\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for infinite scroll page\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import selenium\n",
    "import sys\n",
    "\n",
    "import unittest, time, re\n",
    "\n",
    "# to divert selenium log stream away\n",
    "logging.getLogger('selenium').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )\n",
    "    log.debug(f'Saved to DB')\n",
    "\n",
    "def scrape(url, db, collection):\n",
    "    '''\n",
    "    Scrapes an article from the 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    date        : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Article's html and features stored to db, \n",
    "    Article's publish date\n",
    "    \n",
    "    '''\n",
    "    log.debug(f\"Exctracting features from {url}\")\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        # the below method may only extract a snippet... \n",
    "        # check the database for results of text extraction\n",
    "        # and apply additional processing if needed after \n",
    "        # article has been stored in the DB\n",
    "        # see code below Newrepublic for example\n",
    "        article.parse()\n",
    "    except Exception as e:\n",
    "        log.critical(f'Data not saved: {e}')\n",
    "        return datetime.datetime.now()\n",
    "    \n",
    "    saveToDB(db, collection, article.url, article.html, meta={\n",
    "        'date'    :article.publish_date,\n",
    "        'title'   :article.title,\n",
    "        'text'    :article.text,\n",
    "        'authors' :article.authors\n",
    "    })\n",
    "    \n",
    "    return article.publish_date\n",
    "\n",
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'fiscaltimes'\n",
    "source = 'http://www.thefiscaltimes.com/home?page='\n",
    "page = 1\n",
    "scraped_urls = []\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "\n",
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for h3 in soup.findAll('h3'):\n",
    "        url = urljoin (source, h3.a['href'])\n",
    "        \n",
    "        if url and url not in scraped_urls:\n",
    "            scraped_urls.append(url)\n",
    "            log.debug(f'Processing url: {url}')\n",
    "            article_date = scrape(url, db, collection)\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
