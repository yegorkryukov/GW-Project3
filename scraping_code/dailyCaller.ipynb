{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# For debugging turn on logging to console\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# mongodb\n",
    "import pymongo as pm\n",
    "\n",
    "# fine-tuned newspaper lib\n",
    "from resources.newspaper import newspaper\n",
    "from resources.newspaper.newspaper.source import Source\n",
    "from resources.newspaper.newspaper.article import Article\n",
    "\n",
    "import bs4 as bs\n",
    "from urllib.parse import urljoin\n",
    "from dateutil.parser import parse as date_parser\n",
    "from time import sleep\n",
    "import random\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pm.MongoClient(conn, maxPoolSize=200)\n",
    "\n",
    "# define db \n",
    "# DB_NAME = 'scrape'\n",
    "DB_NAME = 'FINALP'\n",
    "db = client[DB_NAME]\n",
    "\n",
    "def saveToDB(db, collection, url, html, meta={}):\n",
    "    \"\"\"\n",
    "    Saves a document to mongoDB, making sure there are no duplicates by \n",
    "    'url' value\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    db, collection  : mongo db connection\n",
    "    url, html, meta : values to store\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Saved document\n",
    "    \"\"\"\n",
    "    collection = db[collection]\n",
    "    collection.update_one(\n",
    "        {'url' : url},\n",
    "        {\n",
    "            '$set':\n",
    "                {'url' : url,\n",
    "                 'html' : html,\n",
    "                 'meta' : meta\n",
    "                }\n",
    "        }\n",
    "        ,\n",
    "        upsert=True\n",
    "    )\n",
    "    log.debug(f'Saved to DB')\n",
    "\n",
    "def scrape(url, db, collection):\n",
    "    '''\n",
    "    Scrapes an article from the 'url' up to the 'latest_date'\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    url         : main news website url\n",
    "    date        : YYYY-MM-DD\n",
    "    db          : database name\n",
    "    collection  : mongodb collection\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Article's html and features stored to db, \n",
    "    Article's publish date\n",
    "    \n",
    "    '''\n",
    "    log.debug(f\"Exctracting features from {url}\")\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        # the below method may only extract a snippet... \n",
    "        # check the database for results of text extraction\n",
    "        # and apply additional processing if needed after \n",
    "        # article has been stored in the DB\n",
    "        # see code below Newrepublic for example\n",
    "        article.parse()\n",
    "    except Exception as e:\n",
    "        log.critical(f'Data not saved: {e}')\n",
    "        return datetime.datetime.now()\n",
    "    \n",
    "    saveToDB(db, collection, article.url, article.html, meta={\n",
    "        'date'    :article.publish_date,\n",
    "        'title'   :article.title,\n",
    "        'text'    :article.text,\n",
    "        'authors' :article.authors\n",
    "    })\n",
    "    \n",
    "    return article.publish_date\n",
    "\n",
    "def addToDB(DB_NAME,COL_NAME,PATH,FILE):\n",
    "    '''\n",
    "    Imports a file into mongoDB\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    DB_NAME : Name of the database to connect to\n",
    "    COL_NAME: Name of the collection to create\n",
    "    PATH    : Path to folder with the file\n",
    "    FILE  : Filename\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Collection COL_NAME in DB_NAME database\n",
    "    '''\n",
    "    !mongoimport --db {DB_NAME} --collection {COL_NAME} --file {PATH+FILE} --batchSize 1\n",
    "    print(f'Collection {COL_NAME} in {DB_NAME} database created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = 'dailycaller'\n",
    "source = 'http://dailycaller.com/page/'\n",
    "base_url = 'http://dailycaller.com'\n",
    "page   = 1\n",
    "\n",
    "utc=pytz.UTC\n",
    "earliest_date = date_parser('2017-01-01')\n",
    "scraped_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    log.debug(f'\\n\\n PROCESSING PAGE: {source+str(page)}\\n\\n\\\n",
    "              ====================================\\n\\n')\n",
    "    s = Source(source+str(page))\n",
    "    page += 1\n",
    "    s.download()\n",
    "    soup = bs.BeautifulSoup(s.html,'lxml')\n",
    "\n",
    "    # line below needs to be updated per news source\n",
    "    # to include the specific tags for article text \n",
    "    # defined differently for each site\n",
    "    for article in soup.findAll('article'):\n",
    "        url = urljoin(base_url, article.a['href'])\n",
    "        if url and (url not in scraped_urls) and (base_url in url):\n",
    "            log.debug(f'Processing url: {url}')\n",
    "        \n",
    "            article_date = scrape(url, db, collection)\n",
    "\n",
    "    try:\n",
    "        if article_date < earliest_date:\n",
    "            log.debug(f'Reached earliest date requested: {article_date}')\n",
    "            break\n",
    "    except Exception as e:\n",
    "        log.debug(f'Exception: {e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
