{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import titles using Pickle</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pickle\n",
    "\n",
    "def ReadFile(file_name, title_list):\n",
    "    \n",
    "    # Open the files ('rb' is for read binary)\n",
    "    file_object = open(file_name,'rb')\n",
    "    \n",
    "    # Load files into list using pickle\n",
    "    in_list = pickle.load(file_object)\n",
    "    \n",
    "    # Close files\n",
    "    file_object.close()\n",
    "    \n",
    "    # Add list just read in to existing list\n",
    "    title_list.extend(in_list)\n",
    "    \n",
    "    return(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83074\n",
      "105703\n"
     ]
    }
   ],
   "source": [
    "# Crate empty list for left sites\n",
    "left_title_list = []\n",
    "\n",
    "# Read left sites\n",
    "left_title_list = ReadFile(\"data/atlantic_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/BBC_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/mjones_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/newrepublic_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/politico_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/slate_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/thedailybeast_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/theguardian_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/theintercept_titles\", left_title_list)\n",
    "left_title_list = ReadFile(\"data/washpost_titles\", left_title_list)\n",
    "\n",
    "#***MAKE SURE THIS NUMBER MATCHES THE NUMBER OF DOCUMENTS IN MONGODB***\n",
    "print(len(left_title_list))\n",
    "\n",
    "# Crate empty list for right sites\n",
    "right_title_list = []\n",
    "\n",
    "# Read right sites\n",
    "right_title_list = ReadFile(\"data/americanconservative_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/breitbart_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/dailywire_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/economist_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/fiscaltimes_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/foxnews_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/nypost_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/reason_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/thehill_titles\", right_title_list)\n",
    "right_title_list = ReadFile(\"data/washtimes_titles\", right_title_list)\n",
    "\n",
    "#***MAKE SURE THIS NUMBER MATCHES THE NUMBER OF DOCUMENTS IN MONGODB***\n",
    "print(len(right_title_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Add Site Bias to Dataframes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bias                                              title\n",
      "0     1  Conservative High Schoolers Want to ‘Own the L...\n",
      "1     1  The Instagram Forums Where Teens Go to Debate ...\n",
      "2     1  The Doomed Republican Attempt to Impeach Rod R...\n",
      "3     1                          Facebook Is Probably Fine\n",
      "4     1                  Secretary of a State of Confusion\n",
      "(83074, 2)\n",
      "\n",
      "   bias                                        title\n",
      "0     0               Fruits Of The Quiet Revolution\n",
      "1     0               Trust And Mistrust In Churches\n",
      "2     0   A Democratic President From Trump Country?\n",
      "3     0  ‘Arab NATO’: A Terrible Idea That Won’t Die\n",
      "4     0                       TAC Fall Intern Wanted\n",
      "(105703, 2)\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bias of 1 -> Left; Bias of 0 -> Right\n",
    "df1 = pd.DataFrame({'title': np.array(left_title_list), 'bias': 1})\n",
    "df2 = pd.DataFrame({'title': np.array(right_title_list), 'bias': 0})\n",
    "\n",
    "print(df1.head())\n",
    "print(df1.shape)\n",
    "print()\n",
    "print(df2.head())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Downsample Larger Dataframe</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83074, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df2_downsampled = resample(df2, replace = False, n_samples = len(df1), random_state = 41)\n",
    "print(df2_downsampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Combine Left & Right Dataframes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(166148, 2)\n",
      "(166104, 2)\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([df1, df2_downsampled])\n",
    "print()\n",
    "print(df_combined.shape)\n",
    "\n",
    "df_clean = df_combined.dropna()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression on Words in Title</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770095843568\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a matrix of word count using CountVectorizer\n",
    "# Count single words and word pairs (ngram_range = 1-2)\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "# Fit and transform data\n",
    "X = vectorizer.fit_transform(df_clean['title'])\n",
    "\n",
    "# Create training and test split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, df_clean['bias'], random_state=41)\n",
    "\n",
    "# Create the model and fit the training data\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "# Show model's score\n",
    "print(logreg.score(X_test,y_test))\n",
    "\n",
    "coef_zip = zip(logreg.coef_.tolist()[0],list(vectorizer.get_feature_names()))\n",
    "coef_list = list(coef_zip)\n",
    "coef_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-4.201022431474571, 'brickbat') (4.644488353734187, 'washington post')\n",
      "(-3.8137082270899363, 'delingpole') (3.5780070141897897, 'mother jones')\n",
      "(-3.579079684309457, 'walsh') (3.216603867620234, 'redskins')\n",
      "(-3.4999568397248004, 'politics week') (2.8735655961584454, 'pm update')\n",
      "(-3.4306410992089322, 'dem') (2.0908232824740924, 'nationals')\n",
      "(-3.2790485723085196, 'leftists') (1.9957339358999349, 'profile')\n",
      "(-2.735621439431218, 'shapiro') (1.9608445540135495, 'undocumented')\n",
      "(-2.722592505575183, 'nolte') (1.9209226749549264, 'trump舗s')\n",
      "(-2.6915088127663056, 'erickson') (1.8137528591723262, 'country profile')\n",
      "(-2.679696421115009, 'exclusive') (1.7834956771113304, 'conservative media')\n",
      "(-2.6502628379121314, 'limbaugh') (1.7715316587670067, 'slate')\n",
      "(-2.5990304600871337, 'business week') (1.7499378203472362, 'jobs report')\n",
      "(-2.5920847109673684, 'malkin') (1.6739937993857397, 'lawsuit claims')\n",
      "(-2.457779498939968, 'prageru') (1.62597318316157, 'letter africa')\n",
      "(-2.4552966350340277, 'virgil') (1.6148217141674652, 'syria war')\n",
      "(-2.436004881268572, 'wapo') (1.6104256497843001, 'anti muslim')\n",
      "(-2.4263527534336653, 'amnesty') (1.609232577041998, 'happened')\n",
      "(-2.3718879677221056, 'gop senator') (1.6046129330745134, 'annotated')\n",
      "(-2.3588631341598587, 'wh') (1.5999689697484063, 'intercepted')\n",
      "(-2.310426817294341, 'pollak') (1.5953281206858232, 'intercepted podcast')\n"
     ]
    }
   ],
   "source": [
    "# Show top 20 words/phrases for right and left\n",
    "for i in range(0,20):\n",
    "    print(coef_list[i], coef_list[len(coef_list) - i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2.8670418502121082, 'islamic state') (4.746869180273421, 'mother jones')\n",
      "(-2.7377216207281942, 'dem senator') (4.307260082443813, 'country profile')\n",
      "(-2.4200407355194113, 'bozell graham') (3.101874051225947, 'the minute')\n",
      "(-2.384968191535314, 'illegal immigrant') (2.8427003928711514, 'stephen colbert')\n",
      "(-2.372359970152218, 'report trump') (2.688193593697761, 'seth meyers')\n",
      "(-2.3469184350716707, 'gop senator') (2.495900608357429, 'meghan markle')\n",
      "(-2.2050028779278397, 'dem lawmaker') (2.4836027575380406, 'intercepted podcast')\n",
      "(-2.160547051497032, 'illegal aliens') (2.3165487252252515, 'inside the')\n",
      "(-2.1292422048128623, 'dem rep') (2.2702031735942354, 'radio atlantic')\n",
      "(-2.067727731028444, 'gop rep') (2.151885911653669, 'after dark')\n",
      "(-2.063520016802268, 'watch cnn') (2.0913745973395876, 'why did')\n",
      "(-2.0587244679423025, 'illegal alien') (2.0836165270138154, 'russia inquiry')\n",
      "(-2.055302303753218, 'the latest') (1.9934458500964183, 'syria war')\n",
      "(-2.021334234913922, 'anti israel') (1.9722995752452865, 'the secret')\n",
      "(-1.9560160828703137, 'police say') (1.9703746095151302, 'with this')\n",
      "(-1.9458998285823996, 'open borders') (1.9503182590410457, 'michael flynn')\n",
      "(-1.9215320937424532, 'kal cartoon') (1.9433484612669767, 'family separations')\n",
      "(-1.9131961242146829, 'prageru video') (1.9421740521747362, 'why do')\n",
      "(-1.9084185486306808, 'maxine waters') (1.9232150466844566, 'the gop')\n",
      "(-1.859333798916275, 'illegal immigrants') (1.918295176193768, 'the republican')\n"
     ]
    }
   ],
   "source": [
    "# Show top 20 words/phrases for right and left\n",
    "for i in range(0,20):\n",
    "    print(coef_list[i], coef_list[len(coef_list) - i - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22263179,  0.77736821]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(vectorizer.transform(['syria war']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a matrix of word count using CountVectorizer\n",
    "# Count single words and word pairs (ngram_range = 1-2)\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(2,3))\n",
    "\n",
    "# Fit and transform data\n",
    "X = vectorizer.fit_transform(df_combined['title'])\n",
    "\n",
    "# Create training and test split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, df_combined['bias'], random_state=42, \n",
    "                                                    stratify=df_combined['bias'])\n",
    "\n",
    "# Create the model and fit the training data\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#logreg.score(X_test,y_test)\n",
    "\n",
    "a = zip(logreg.coef_.tolist()[0],list(vectorizer.get_feature_names()))\n",
    "b = list(a)\n",
    "b.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429053\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Output Model Results to JSON File</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for i in range(0,50):\n",
    "    for item in coef_list[i]:\n",
    "        if type(item) is str:\n",
    "            d[item] = logreg.predict_proba(vectorizer.transform([item]))[0][0]\n",
    "            \n",
    "d\n",
    "            \n",
    "#import json\n",
    "\n",
    "# Write list of dictionaries to JS file\n",
    "#with open ('model-data.json', 'w') as outfile:\n",
    "#    json.dump([d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22263179,  0.77736821]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict_proba(vectorizer.transform(['syria war']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90.550753572625112, 93.301483246755623, 94.016395249112975, 96.443661108029289, 96.712372869410416]\n",
      "[-75.854714214487345, -77.511498328533079, -77.736821015758963, -80.368374524988155, -82.59889590187592]\n"
     ]
    }
   ],
   "source": [
    "right_words = ['2nd amendment', 'daca amnesty', 'open borders', 'illegal immigrant', 'islamic state']\n",
    "\n",
    "right_scores = []\n",
    "\n",
    "for phrase in right_words:\n",
    "    score = logreg.predict_proba(vectorizer.transform([phrase]))[0][0] * 100\n",
    "    right_scores.append(score)\n",
    "    \n",
    "left_words = ['undocumented immigrants', 'muslim ban', 'syria war', 'family separations', 'russia inquiry']\n",
    "\n",
    "left_scores = []\n",
    "\n",
    "for phrase in left_words:\n",
    "    score = -logreg.predict_proba(vectorizer.transform([phrase]))[0][1] * 100\n",
    "    left_scores.append(score)\n",
    "    \n",
    "print(right_scores)\n",
    "print(left_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\spenc\\\\Documents\\\\pattern\\\\joint-bar.html'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "left_text = []\n",
    "for i in range(0,5):\n",
    "    text = left_words[i] + ' ' + str(round(left_scores[i],1)) + '% '\n",
    "    left_text.append(text)\n",
    "    \n",
    "right_text = []\n",
    "for i in range(0,5):\n",
    "    text = right_words[i] + ' ' + str(round(right_scores[i],1)) + '% '\n",
    "    right_text.append(text)\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    y = np.arange(1,6),\n",
    "    x = left_scores,\n",
    "    text = left_text,\n",
    "    textposition = 'auto',\n",
    "    textfont = dict(size = 16, color = 'white'),\n",
    "    hoverinfo = 'none',\n",
    "    name = 'Left',\n",
    "    orientation = 'h',\n",
    "    marker = dict(color = 'blue'),\n",
    "    offset = [-0.5,-1,-1,-1,0]\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    y = np.arange(1,6),\n",
    "    x = right_scores,\n",
    "    text = right_text,\n",
    "    textposition = 'auto',\n",
    "    textfont = dict(size = 16, color = 'white'),\n",
    "    hoverinfo = 'none',\n",
    "    name = 'Right',\n",
    "    orientation = 'h',\n",
    "    marker = dict(color = 'red')\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group',\n",
    "    title = '<b>Model Phrase Probability</b>',\n",
    "    titlefont = dict(\n",
    "        size = 24\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        showticklabels = False\n",
    "    ),\n",
    "    xaxis = dict(\n",
    "        title = 'Probability that article has left/right bias',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename = 'joint-bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file://C:\\\\Users\\\\spenc\\\\Documents\\\\pattern\\\\right-bar.html'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = [trace1]\n",
    "data2 = [trace2]\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group',\n",
    "    title = '<b>Left Bias Probability</b>',\n",
    "    titlefont = dict(\n",
    "        size = 24\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        showticklabels = False\n",
    "    ),\n",
    "    xaxis = dict(\n",
    "        title = 'Probability that article bias is left',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data1, layout=layout)\n",
    "plotly.offline.plot(fig, filename = 'left-bar.html')\n",
    "\n",
    "layout = go.Layout(\n",
    "    barmode='group',\n",
    "    title = '<b>Right Bias Probability</b>',\n",
    "    titlefont = dict(\n",
    "        size = 24\n",
    "    ),\n",
    "    yaxis = dict(\n",
    "        showticklabels = False\n",
    "    ),\n",
    "    xaxis = dict(\n",
    "        title = 'Probability that article bias is right',\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data2, layout=layout)\n",
    "plotly.offline.plot(fig, filename = 'right-bar.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
